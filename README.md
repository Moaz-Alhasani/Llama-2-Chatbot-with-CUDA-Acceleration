# Llama-2-Chatbot-with-CUDA-Acceleration

# 📖 Description:
This project utilizes Llama-2 13B GGML to run a local chatbot with CUDA acceleration via llama-cpp-python. It enables large language model inference on GPU using cublas, improving performance and efficiency.

# 🔧 Requirements:
  Python 3.8+
  CUDA-enabled GPU
  llama-cpp-python
  huggingface_hub
  numpy
# ✅ Features:
  🚀 Runs Llama-2 on GPU using cublas for acceleration
  📥 Loads the model directly from Hugging Face Hub
  🛠️ Easy-to-use API via llama-cpp-python
# 🛠 Contributors:
###    Mohammad Moaz Al-Hasani
