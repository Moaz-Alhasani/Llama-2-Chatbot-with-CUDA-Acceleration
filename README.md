# Llama-2-Chatbot-with-CUDA-Acceleration

# ğŸ“– Description:
This project utilizes Llama-2 13B GGML to run a local chatbot with CUDA acceleration via llama-cpp-python. It enables large language model inference on GPU using cublas, improving performance and efficiency.

# ğŸ”§ Requirements:
  Python 3.8+
  CUDA-enabled GPU
  llama-cpp-python
  huggingface_hub
  numpy
# âœ… Features:
  ğŸš€ Runs Llama-2 on GPU using cublas for acceleration
  ğŸ“¥ Loads the model directly from Hugging Face Hub
  ğŸ› ï¸ Easy-to-use API via llama-cpp-python
# ğŸ›  Contributors:
###    Mohammad Moaz Al-Hasani
